// src/services/crawler.js - Áà¨Ëô´ÊúçÂä°
const db = require('../database/db')
const cron = require('node-cron')
const RedpackCrawler = require('../crawler/redpack')

class CrawlerService {
  constructor() {
    this.scheduledTasks = new Map() // Â≠òÂÇ®Â∑≤Ë∞ÉÂ∫¶ÁöÑ‰ªªÂä°
  }

  /**
   * ÂàùÂßãÂåñË∞ÉÂ∫¶Âô®
   */
  async init() {
    const tasks = await this.getActiveTasks()

    for (const task of tasks) {
      this.scheduleTask(task)
    }

    console.log(`‚úÖ Áà¨Ëô´Ë∞ÉÂ∫¶Âô®Â∑≤ÂêØÂä®ÔºåÊ¥ªË∑É‰ªªÂä°Êï∞: ${this.scheduledTasks.size}`)
  }

  /**
   * Ëé∑ÂèñÊ¥ªË∑É‰ªªÂä°
   */
  async getActiveTasks() {
    const tasks = await db.query(
      'SELECT * FROM crawler_tasks WHERE status = "active"'
    )
    return tasks
  }

  /**
   * Ë∞ÉÂ∫¶Âçï‰∏™‰ªªÂä°
   */
  scheduleTask(task) {
    const { id, cron_expression, type } = task

    try {
      // È™åËØÅ cron Ë°®ËææÂºè
      if (!cron.validate(cron_expression)) {
        console.error(`‚ùå Áà¨Ëô´‰ªªÂä° ${task.name} ÁöÑ cron Ë°®ËææÂºèÊó†Êïà: ${cron_expression}`)
        return
      }

      const cronTask = cron.schedule(cron_expression, async () => {
        await this.executeTask(id)
      }, {
        scheduled: false
      })

      this.scheduledTasks.set(id, cronTask)
      cronTask.start()

      console.log(`üìã Áà¨Ëô´‰ªªÂä°Â∑≤Ë∞ÉÂ∫¶: ${task.name} (${cron_expression})`)
    } catch (error) {
      console.error(`‚ùå Ë∞ÉÂ∫¶Áà¨Ëô´‰ªªÂä°Â§±Ë¥•:`, error)
    }
  }

  /**
   * ÊâßË°åÁà¨Ëô´‰ªªÂä°
   */
  async executeTask(taskId) {
    const connection = await db.pool.getConnection()
    let logId

    try {
      // Ëé∑Âèñ‰ªªÂä°‰ø°ÊÅØ
      const tasks = await db.query(
        'SELECT * FROM crawler_tasks WHERE id = ?',
        [taskId]
      )

      if (tasks.length === 0) {
        throw new Error('‰ªªÂä°‰∏çÂ≠òÂú®')
      }

      const task = tasks[0]

      // Ê£ÄÊü•‰ªªÂä°Áä∂ÊÄÅ
      if (task.status !== 'active') {
        console.log(`‚è∏Ô∏è ‰ªªÂä° ${task.name} Êú™ÊøÄÊ¥ªÔºåË∑≥ËøáÊâßË°å`)
        return
      }

      // ÂàõÂª∫ÊâßË°åÊó•Âøó
      const [logResult] = await connection.query(
        'INSERT INTO crawler_logs (task_id, start_time, status) VALUES (?, NOW(), "running")',
        [taskId]
      )
      logId = logResult.insertId

      console.log(`üöÄ ÂºÄÂßãÊâßË°åÁà¨Ëô´‰ªªÂä°: ${task.name}`)

      // ÊâßË°åÁà¨Ëô´
      const crawler = this.getCrawler(task.type)
      const result = await crawler.run({
        sourceUrl: task.source_url,
        config: task.config
      })

      // Êõ¥Êñ∞Êó•Âøó
      await connection.query(
        `UPDATE crawler_logs
         SET end_time = NOW(),
             status = 'success',
             items_fetched = ?,
             items_created = ?,
             items_updated = ?
         WHERE id = ?`,
        [result.fetched || 0, result.created || 0, result.updated || 0, logId]
      )

      // Êõ¥Êñ∞‰ªªÂä°ÁªüËÆ°
      await connection.query(
        `UPDATE crawler_tasks
         SET last_run_at = NOW(),
             success_count = success_count + 1
         WHERE id = ?`,
        [taskId]
      )

      console.log(`‚úÖ Áà¨Ëô´‰ªªÂä° ${task.name} ÊâßË°åÊàêÂäü: Êñ∞Â¢û${result.created}Êù°ÔºåÊõ¥Êñ∞${result.updated}Êù°`)

    } catch (error) {
      console.error(`‚ùå Áà¨Ëô´‰ªªÂä°ÊâßË°åÂ§±Ë¥•:`, error)

      await connection.query(
        `UPDATE crawler_logs
         SET end_time = NOW(),
             status = 'failed',
             error_message = ?
         WHERE id = ?`,
        [error.message, logId]
      )

      await connection.query(
        'UPDATE crawler_tasks SET fail_count = fail_count + 1 WHERE id = ?',
        [taskId]
      )

    } finally {
      connection.release()
    }
  }

  /**
   * Ëé∑ÂèñÁà¨Ëô´ÂÆû‰æã
   */
  getCrawler(type) {
    const crawlers = {
      'redpack': RedpackCrawler
    }
    return crawlers[type]
  }

  /**
   * Ëé∑Âèñ‰ªªÂä°ÂàóË°®
   */
  async getList(page = 1, pageSize = 20) {
    const offset = (page - 1) * pageSize

    const list = await db.query(
      `SELECT * FROM crawler_tasks
       ORDER BY created_at DESC
       LIMIT ? OFFSET ?`,
      [pageSize, offset]
    )

    const countResult = await db.query('SELECT COUNT(*) as total FROM crawler_tasks')

    return {
      list,
      total: countResult[0].total,
      page,
      pageSize
    }
  }

  /**
   * Ëé∑Âèñ‰ªªÂä°ËØ¶ÊÉÖ
   */
  async getDetail(id) {
    const tasks = await db.query('SELECT * FROM crawler_tasks WHERE id = ?', [id])

    if (tasks.length === 0) {
      throw new Error('‰ªªÂä°‰∏çÂ≠òÂú®')
    }

    return tasks[0]
  }

  /**
   * ÂàõÂª∫‰ªªÂä°
   */
  async create(data) {
    const { name, type, sourceUrl, cronExpression, config } = data

    // È™åËØÅ cron Ë°®ËææÂºè
    if (!cron.validate(cronExpression)) {
      throw new Error('Cron Ë°®ËææÂºèÊó†Êïà')
    }

    const result = await db.query(
      `INSERT INTO crawler_tasks (name, type, source_url, cron_expression, config)
       VALUES (?, ?, ?, ?, ?)`,
      [name, type, sourceUrl, cronExpression, JSON.stringify(config || {})]
    )

    // Ëé∑ÂèñÂàõÂª∫ÁöÑ‰ªªÂä°
    const tasks = await db.query('SELECT * FROM crawler_tasks WHERE id = ?', [result.insertId])

    // Â¶ÇÊûúÁä∂ÊÄÅÊòØ activeÔºåÁ´ãÂç≥Ë∞ÉÂ∫¶
    if (tasks[0].status === 'active') {
      this.scheduleTask(tasks[0])
    }

    return tasks[0]
  }

  /**
   * Êõ¥Êñ∞‰ªªÂä°
   */
  async update(id, data) {
    const { name, sourceUrl, cronExpression, config, status } = data

    // È™åËØÅ cron Ë°®ËææÂºè
    if (cronExpression && !cron.validate(cronExpression)) {
      throw new Error('Cron Ë°®ËææÂºèÊó†Êïà')
    }

    // ÊöÇÂÅúÁé∞ÊúâË∞ÉÂ∫¶
    this.pauseTask(id)

    const updateFields = []
    const values = []

    if (name) {
      updateFields.push('name = ?')
      values.push(name)
    }
    if (sourceUrl) {
      updateFields.push('source_url = ?')
      values.push(sourceUrl)
    }
    if (cronExpression) {
      updateFields.push('cron_expression = ?')
      values.push(cronExpression)
    }
    if (config) {
      updateFields.push('config = ?')
      values.push(JSON.stringify(config))
    }
    if (status) {
      updateFields.push('status = ?')
      values.push(status)
    }

    values.push(id)

    await db.query(
      `UPDATE crawler_tasks SET ${updateFields.join(', ')} WHERE id = ?`,
      values
    )

    // Ëé∑ÂèñÊõ¥Êñ∞ÂêéÁöÑ‰ªªÂä°
    const tasks = await db.query('SELECT * FROM crawler_tasks WHERE id = ?', [id])

    // Â¶ÇÊûúÁä∂ÊÄÅÊòØ activeÔºåÈáçÊñ∞Ë∞ÉÂ∫¶
    if (tasks[0].status === 'active') {
      this.scheduleTask(tasks[0])
    }

    return tasks[0]
  }

  /**
   * Âà†Èô§‰ªªÂä°
   */
  async delete(id) {
    // ÊöÇÂÅú‰ªªÂä°
    this.pauseTask(id)

    await db.query('DELETE FROM crawler_tasks WHERE id = ?', [id])
    return true
  }

  /**
   * ÊâãÂä®Ëß¶Âèë‰ªªÂä°
   */
  async manualRun(id) {
    // ÂºÇÊ≠•ÊâßË°åÔºå‰∏çÈòªÂ°û
    this.executeTask(id).catch(error => {
      console.error('ÊâãÂä®Ëß¶Âèë‰ªªÂä°Â§±Ë¥•:', error)
    })

    return { message: '‰ªªÂä°Â∑≤ÂêØÂä®' }
  }

  /**
   * ÊöÇÂÅú‰ªªÂä°
   */
  pauseTask(id) {
    const task = this.scheduledTasks.get(id)
    if (task) {
      task.stop()
      this.scheduledTasks.delete(id)
    }
  }

  /**
   * ÊÅ¢Â§ç‰ªªÂä°
   */
  async resumeTask(id) {
    const tasks = await db.query('SELECT * FROM crawler_tasks WHERE id = ?', [id])

    if (tasks.length > 0 && tasks[0].status === 'active') {
      this.scheduleTask(tasks[0])
    }
  }

  /**
   * Ëé∑ÂèñÊâßË°åÊó•Âøó
   */
  async getLogs(taskId, page = 1, pageSize = 20) {
    const offset = (page - 1) * pageSize

    const list = await db.query(
      `SELECT * FROM crawler_logs
       WHERE task_id = ?
       ORDER BY start_time DESC
       LIMIT ? OFFSET ?`,
      [taskId, pageSize, offset]
    )

    const countResult = await db.query(
      'SELECT COUNT(*) as total FROM crawler_logs WHERE task_id = ?',
      [taskId]
    )

    return {
      list,
      total: countResult[0].total,
      page,
      pageSize
    }
  }
}

module.exports = new CrawlerService()
